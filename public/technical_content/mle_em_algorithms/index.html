<!DOCTYPE html>
<html><!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <title>Sebastian Sciarra - Probability, Likelihood, and Maximum Likelihood Estimation</title>

  <meta name="author" content="Sebastian Sciarra">
        <link rel="stylesheet" href="/css/webStyling.css"/>
        <link rel="stylesheet" href="/css/webStyling.min.abbd1066440353ad4b1f0d345757263e74fb4c175e680cf29d7ec16b45828f3d.css">

  <link rel="stylesheet" type="text/css" href="/hugo-cite.css" />
  <meta name="description" content="Seb&#39;s portfolio site">

 
  


</head>


<body>

      <div class="container"><!DOCTYPE html>




  
   <script src="https://kit.fontawesome.com/23dde744ab.js" crossorigin="anonymous"></script>

   
    <script type = 'text/javascript'>

      function active() {
         document.getElementsByClassName('navbar-links')[0].classList.toggle('active');
      }

    </script>


    <div class = 'navbar'>
       <h1 class = 'blog_name'>Blog Directory</h1>

       <ul class = 'navbar-links'>
        <li><a class = 'home', href = "/home"><i class="fa-solid fa-house"></i> Home</a></li>
        <li><a class = 'technical', href = "/technical_content"><i class="fa-solid fa-gears"></i> Technical Content</a></li>
        <li><a class = 'coding', href = "/coding_tricks"><i class="fa-solid fa-code"></i> Coding Demos & Tricks</a></li>
        <li><a class = 'simulation', href = "/simulation_exps"><i class="fa-brands fa-buromobelexperte"></i> Simulation Experiments</a></li>
        <li><a class = 'resources', href = "/mlresources"><i class="fa-solid fa-key"></i> ML Resources</a></li>
        <li><a class = 'about', href = "/about"><i class="fa-solid fa-question"></i> About</a></li>
        <li><a class = 'portal', href = "/"><i class="fa-solid fa-circle-chevron-left"></i> back to portal</a></li>
       </ul>

      <button hef = '#', class = "toggle-button",  onclick="active()">
        <span class = "bar"> </span>
        <span class = "bar"> </span>
        <span class = "bar"> </span>
      </button>


  </div>





<link rel="alternate" type="application/rss+xml" href="http://example.com/feed" >





<main>
<div class = 'blog_post'>


  <div class = 'post'>

     <div class = 'header technical'>

        <h2 class = 'blog_title'> Probability, Likelihood, and Maximum Likelihood Estimation </h2>

        <div class = 'blog_summary'> Explanation of post </div>

         <br>

        <time> <b> Published </b> <br>
        9 March 2023
        </time>

    </div>


    <div class = 'content'>
      <h1 id="probability-mass-functions-the-probability-of-observing-each-possible-outcome-given-one-set-of-parameter-values">Probability Mass Functions: The Probability of Observing Each Possible Outcome Given One Set of Parameter Values</h1>
<p>Consider an example where a researcher obtains a coin and believes it to be unbiased, $P(\theta) = P(head) = 0.50$. To test this hypothesis, the researcher intends to flip the coin 10 times and record the result as a <code>1</code> for heads and <code>0</code> for tails, thus obtaining a vector of 10 observed scores, $\mathbf{y} \in \{0, 1 \}^{10}$, where $n = 10$. Before collecting the data to test their hypothesis, the researcher would like to get an idea of the probability of observing any given number of heads given that the coin is unbiased and there are 10 coin flips, $P(\mathbf{y}|\theta, n)$. Thus, the outcome of interest is the number of heads, $h$, where $\{h|0 \le h \le10\}$. Because the coin flips have a dichotomous outcome and the result of any given flip is independent of all the other flips, the probability of obtaining any given number of heads will be distributed according to a binomial distribution, $h \sim B(n, h)$. To compute the probability of obtaining any given number of heads, the <em>binomial function</em> shown below in Equation \ref{eq:prob-mass-function} can be used:
$$
\begin{align}
P(h|\theta, n) = {n \choose h}(\theta)^{h}(1-\theta)^{(n-h)},
\label{eq:prob-mass-function}
\end{align}
$$
where ${n \choose h}$ gives the total number of ways in which $h$ heads (or successes) can be obtained in a series of $n$ attempts (i.e., coin flips) and $(\theta)^{h}(1-\theta)^{(n-h)}$ gives the probability of obtaining a given number of $h$ heads and $n-h$ tails in a given set of $n$ flips. Thus, the binomial function (Equation \ref{eq:prob-mass-function}) has an underlying intuition: To compute the probability of obtaining a given number of $h$ heads given $n$ flips and a certain $\theta$ probability of success, the probability of obtaining $h$ heads in a given set of $n$ coin flips, $(\theta)^{h}(1-\theta)^{(n-h)}$, is multiplied by the total number of ways in which $h$ heads can be obtained in $n$ coin flips ${n \choose h}$.</p>
<p>As an example, the probability of obtaining four heads ($h=4$) in 10 coin flips ($n = 10$) is calculated below.</p>
<p>$$
\begin{alignat}{2}
P(h = 4|\theta = 0.50, n = 10) &amp;= {10 \choose 4}(0.50)^{4}(1-0.50)^{(10-4)}   \nonumber \\
&amp;= \frac{10!}{4! (10 - 4)!}(0.50)^{4}(1-0.50)^{(10-4)} \nonumber \\
&amp;= 210(0.5)^{10}\nonumber \\
&amp;= 0.205 \nonumber
\end{alignat}
$$
Thus, there are 210 possible ways of obtaining four heads in a series of 10 coin flips, with each way having a probability of $(0.5)^{10}$ of occurring. Altogether, four heads have a probability of .205 of occurring given a probability of heads of .50 and 10 coin flips.</p>
<p>In order to calculate the probability of obtaining each possible number of heads in a series of 10 coin flips, the binomial function (Equation \ref{eq:prob-mass-function}) can be computed for each number. The resulting probabilities of obtaining each number of heads can then be plotted to produce a <em>probability mass function</em>: A distribution that gives the probability of obtaining each possible value of a discrete random variable<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> (see Figure \ref{fig:prob-mass-binom}). Importantly, probability mass functions have two conditions: 1) the probability of obtaining each value is non-negative and 2) the sum of all probabilities is zero. The R code block below (lines <a href="#1">1&ndash;68</a>) produces a probability mass function for the binomial situation.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1">#create function that computes probability mass function with following arguments:</span>
</span></span><span class="line"><span class="cl">  <span class="c1">##num_trials = number of trials (10  [coin flips] in the current example)</span>
</span></span><span class="line"><span class="cl">  <span class="c1">##prob_success = probability of success (or heads; 0.50 in the current example)</span>
</span></span><span class="line"><span class="cl">  <span class="c1">##num_successes = number of successes (or heads; [1-10] in the current example)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">compute_binom_mass_density</span> <span class="o">&lt;-</span> <span class="nf">function</span><span class="p">(</span><span class="n">num_trials</span><span class="p">,</span> <span class="n">prob_success</span><span class="p">,</span> <span class="n">num_successes</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">#computation of binomial term (i.e., number of ways of obtaining a given number of successes)</span>
</span></span><span class="line"><span class="cl">  <span class="n">num_success_patterns</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="nf">factorial</span><span class="p">(</span><span class="n">num_trials</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nf">factorial</span><span class="p">(</span><span class="n">num_successes</span><span class="p">)</span><span class="o">*</span><span class="nf">factorial</span><span class="p">(</span><span class="n">num_trials</span><span class="o">-</span><span class="n">num_successes</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">#computation of the number of possible ways of obtaining a given number of successes (i.e., heads)</span>
</span></span><span class="line"><span class="cl">  <span class="n">prob_single_pattern</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">prob_success</span><span class="p">)</span><span class="n">^num_successes</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="n">prob_success</span><span class="p">)</span><span class="nf">^</span><span class="p">(</span><span class="n">num_trials</span><span class="o">-</span><span class="n">num_successes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">probability</span> <span class="o">&lt;-</span> <span class="n">num_success_patterns</span><span class="o">*</span><span class="n">prob_single_pattern</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">pmf_df</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span><span class="s">&#39;probability&#39;</span> <span class="o">=</span> <span class="n">probability</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                   <span class="s">&#39;num_successes&#39;</span> <span class="o">=</span> <span class="n">num_successes</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                   <span class="s">&#39;prob_success&#39;</span> <span class="o">=</span> <span class="n">prob_success</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                   <span class="s">&#39;num_trials&#39;</span> <span class="o">=</span> <span class="n">num_trials</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="nf">return</span><span class="p">(</span><span class="n">pmf_df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">num_trials</span> <span class="o">&lt;-</span> <span class="m">10</span>
</span></span><span class="line"><span class="cl"><span class="n">prob_success</span> <span class="o">&lt;-</span> <span class="m">0.5</span>
</span></span><span class="line"><span class="cl"><span class="n">num_successes</span> <span class="o">&lt;-</span> <span class="m">0</span><span class="o">:</span><span class="m">10</span>  <span class="c1">#manipulated variable </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">prob_distribution</span> <span class="o">&lt;-</span> <span class="nf">compute_binom_mass_density</span><span class="p">(</span><span class="n">num_trials</span><span class="p">,</span> <span class="n">prob_success</span><span class="p">,</span> <span class="n">num_successes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">library </span><span class="p">(</span><span class="n">tidyverse</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">grDevices</span><span class="p">)</span> <span class="c1">#needed for italic()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#create data set for shaded rectangle that indicates the most likely value </span>
</span></span><span class="line"><span class="cl"><span class="c1">##index of highest probability </span>
</span></span><span class="line"><span class="cl"><span class="n">highest_number_ind</span> <span class="o">&lt;-</span> <span class="nf">which.max</span><span class="p">(</span><span class="n">prob_distribution</span><span class="o">$</span><span class="n">probability</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="c1">##most likely number of successes</span>
</span></span><span class="line"><span class="cl"><span class="n">most_likely_number</span> <span class="o">&lt;-</span> <span class="n">prob_distribution</span><span class="o">$</span><span class="n">num_successes[highest_number_ind]</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#create pmf plot </span>
</span></span><span class="line"><span class="cl"><span class="n">pmf_plot</span> <span class="o">&lt;-</span> <span class="nf">ggplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">prob_distribution</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">num_successes</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">probability</span><span class="p">))</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">  <span class="nf">geom_bar</span><span class="p">(</span><span class="n">stat</span> <span class="o">=</span> <span class="s">&#39;identity&#39;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">           <span class="n">fill</span> <span class="o">=</span>  <span class="nf">ifelse</span><span class="p">(</span><span class="n">test</span> <span class="o">=</span> <span class="n">prob_distribution</span><span class="o">$</span><span class="n">num_successes</span> <span class="o">==</span> <span class="n">most_likely_number</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">no</span> <span class="o">=</span>  <span class="s">&#34;#002241&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">yes</span> <span class="o">=</span> <span class="s">&#34;#00182d&#34;</span><span class="p">))</span> <span class="o">+</span>  <span class="c1">#calculate sum of probability for each num_successes</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nf">scale_y_continuous</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="nf">bquote</span><span class="p">(</span><span class="nf">italic</span><span class="p">(</span><span class="s">&#34;P(h&#34;</span><span class="p">)</span><span class="o">*</span><span class="s">&#34;|&#34;</span><span class="o">*</span><span class="nf">italic</span><span class="p">(</span><span class="n">theta</span> <span class="o">==</span> <span class="n">.(prob_success</span><span class="p">)</span><span class="o">*</span><span class="s">&#34;,&#34;</span><span class="o">~</span><span class="n">n</span> <span class="o">==</span> <span class="n">.(num_trials</span><span class="p">)</span><span class="o">*</span><span class="s">&#34;)&#34;</span><span class="p">)))</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">  <span class="nf">scale_x_continuous</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="nf">bquote</span><span class="p">(</span><span class="s">&#34;Number of Heads (i.e., &#34;</span><span class="o">*</span><span class="nf">italic</span><span class="p">(</span><span class="s">&#34;h&#34;</span><span class="p">)</span><span class="o">~</span><span class="s">&#34;)&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                     <span class="n">breaks</span> <span class="o">=</span> <span class="nf">seq</span><span class="p">(</span><span class="n">from</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">to</span> <span class="o">=</span> <span class="m">10</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="m">1</span><span class="p">))</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">theme_classic</span><span class="p">(</span><span class="n">base_family</span> <span class="o">=</span> <span class="s">&#34;Helvetica&#34;</span><span class="p">,</span> <span class="n">base_size</span> <span class="o">=</span> <span class="m">18</span><span class="p">)</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">theme</span><span class="p">(</span><span class="n">axis.title.y</span> <span class="o">=</span> <span class="nf">element_text</span><span class="p">(</span><span class="n">face</span> <span class="o">=</span> <span class="s">&#39;italic&#39;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1">#embolden the most likely number of heads </span>
</span></span><span class="line"><span class="cl">        <span class="n">axis.text.x</span> <span class="o">=</span> 
</span></span><span class="line"><span class="cl">          <span class="nf">element_text</span><span class="p">(</span><span class="n">face</span> <span class="o">=</span> 
</span></span><span class="line"><span class="cl">                         <span class="nf">ifelse</span><span class="p">(</span><span class="n">test</span> <span class="o">=</span> <span class="n">prob_distribution</span><span class="o">$</span><span class="n">num_successes</span> <span class="o">==</span> <span class="n">most_likely_number</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">no</span> <span class="o">=</span>  <span class="s">&#34;plain&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">yes</span> <span class="o">=</span> <span class="s">&#34;bold&#34;</span><span class="p">)),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">text</span> <span class="o">=</span> <span class="nf">element_text</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span> <span class="s">&#34;#002241&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">axis.line</span> <span class="o">=</span> <span class="nf">element_line</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span> <span class="s">&#34;#002241&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">axis.ticks</span> <span class="o">=</span> <span class="nf">element_line</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span>  <span class="s">&#34;#002241&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">axis.text</span> <span class="o">=</span> <span class="nf">element_text</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span> <span class="s">&#34;#002241&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">ggsave</span><span class="p">(</span><span class="n">filename</span> <span class="o">=</span> <span class="s">&#39;images/pmf_plot.png&#39;</span><span class="p">,</span> <span class="n">plot</span> <span class="o">=</span> <span class="n">pmf_plot</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="m">6</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="m">8</span><span class="p">)</span>
</span></span></code></pre></div><div class="figure">
  <div class="figDivLabel">
    <caption>
      <span class = 'figLabel'>Figure \ref{fig:prob-mass-binom}<span> 
    </caption>
  </div>
   <div class="figTitle">
    Probability Mass Function With an Unbiased Coin (<span class = "theta">&theta;</span> = 0.50) and Ten Coin Flips (n = 10)
  </div>
    <img src="images/pmf_plot.png" width="70%" height="70%"> 
  <div class="figNote">
      <span><em>Note. </em> Number emboldened on the x-axis indicates the number of heads that is most likely to occur with an unbiased coin and 10 coin flips, with the corresponding bar in darker blue  indicating the corresponding probability.</span> 
  </div>
</div>
<p>Figure \ref{fig:prob-mass-binom} shows the probability mass function that results with an unbiased coin ($\theta = 0.50$) and ten coin flips ($n = 10$). In looking across the probability values of obtaining each number of heads (x-axis), 5 heads is the most likely value, as indicated by the emboldened number on the x-axis and the bar above it with a darker blue color. As an aside, the R code below verifies the two conditions of probability mass functions for the current example (for a mathematical proof, see <a href="#proof-pmf">Appendix A</a>).</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1">#Condition 1: All probability values have nonnegative values. </span>
</span></span><span class="line"><span class="cl"><span class="nf">sum</span><span class="p">(</span><span class="n">prob_distribution</span><span class="o">$</span><span class="n">probability</span> <span class="o">&gt;=</span> <span class="m">0</span><span class="p">)</span> <span class="c1">#11 nonnegative values </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#Condition 2: Sum of all probability values is 1. </span>
</span></span><span class="line"><span class="cl"><span class="nf">sum</span><span class="p">(</span><span class="n">prob_distribution</span><span class="o">$</span><span class="n">probability</span><span class="p">)</span>  <span class="c1">#1</span>
</span></span></code></pre></div><pre><code class='r-code'>[1] 11
[1] 1
</code></pre>
<p>With a probability mass function that shows the probability of obtaining each possible number of heads, the researcher now has an idea of what outcomes to expect after flipping the coin 10 times. Unfortunately, the probability mass function in Figure \ref{fig:prob-mass-binom} gives no insight into the coin&rsquo;s probability of heads after data have been collected; in computing the probability mass function, the probability of heads ($\theta$) is fixed. Thus, the researcher must use a different type of distribution to estimate the coin&rsquo;s probability of heads.</p>
<h1 id="likelihood-distributions-the-probability-of-observing-each-possible-set-of-parameter-values-given-a-specific-outcome">Likelihood Distributions: The Probability of Observing Each Possible Set of Parameter Values Given a Specific Outcome</h1>
<p>Continuing with the coin flipping example, the researcher flips the coin 10 times and obtains seven heads. With these data, the researcher wants to determine the probability value of heads, $\theta$, that most likely produced the data, $P(h, n|\theta)$<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.  Before continuing, it is important to explain why the researcher is no longer dealing with probabilities and is instead dealing with likelihoods.</p>
<h2 id="likelihoods-are-not-probabilities">Likelihoods are not Probabilities</h2>
<p>Because we are interested in determining which value of $\theta \in [0, 1]$ most likely produced the data, the probability of observing the data must be computed for each of these values, $P(h = 7, n = 10|\theta)$. Thus, we now fix the data, $h = 7, n = 10$, and vary the parameter value of $\theta$. Although we also use the binomial function to compute $P(h = 7, n = 10|\theta)$ for each $\theta \in [0, 1]$, the resulting values are not probabilities because they do not sum to 1. Indeed, the code below shows that the values sum to 9.09. Thus, when fixing the data and varying the parameter values, the resulting values do not sum to one (for a mathematical proof with the binomial function, see <a href="#proof-likelihood">Appendix B</a> and are, therefore, not probabilities: they are likelihoods. To signify the fundamental difference between probabilities and likelihoods, we use different notations.  When fixing the data and varying the parameter values, we compute the likelihood of the parameter given the data, $L(\theta|h, n)$.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">num_trials</span> <span class="o">&lt;-</span> <span class="m">10</span>
</span></span><span class="line"><span class="cl"><span class="n">num_successes</span> <span class="o">&lt;-</span> <span class="m">7</span>
</span></span><span class="line"><span class="cl"><span class="n">prob_success</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="n">from</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">to</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span> <span class="c1">#manipulated variable </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#compute P(h, n|theta) for each theta in [0, 1].</span>
</span></span><span class="line"><span class="cl"><span class="n">likelihood_distribution</span> <span class="o">&lt;-</span> <span class="nf">compute_binom_mass_density</span><span class="p">(</span><span class="n">num_trials</span> <span class="o">=</span> <span class="n">num_trials</span><span class="p">,</span> <span class="n">num_successes</span> <span class="o">=</span>  <span class="n">num_successes</span><span class="p">,</span> <span class="n">prob_success</span> <span class="o">=</span> <span class="n">prob_success</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">sum</span><span class="p">(</span><span class="n">likelihood_distribution</span><span class="o">$</span><span class="n">probability</span><span class="p">)</span>
</span></span></code></pre></div><pre><code class='r-code'>[1] 9.09091
</code></pre>
<p>In computing likelihoods, it is important to note they cannot be interpreted as probabilities. As an example, the likelihood of 0.108 obtained for $L(\theta = .50|h=7, n=10)$ does not mean that, given a probability of heads of .50, there is a 10.80% chance that seven heads will arise in 10 coin flips: The value of $L(\theta = .50|h=7, n=10) = 0.108$ provides a measure of how strongly the data are expected under the hypothesis that $\theta = .50$. To gain a better understanding of whether the likelihood value of 0.108 is a high value, the likelihoods for each value of $\theta$ can be computed.</p>
<h2 id="creating-a-likelihood-distribution-to-find-the-maximum-likelihood-estimate">Creating a Likelihood Distribution to Find the Maximum Likelihood Estimate</h2>
<p>In Figure \ref{fig:likelihood-dist}, I have plotted the likelihood for each value of $\theta \in [0, 1]$. By plotting the likelihoods, the parameter value that most likely produced the data or the <em>maximum likelihood estimate</em> can be identified. The maximum likelihood estimate of $\theta$ is the value of .70, which is emboldened on the x-axis and its likelihood indicated by the vertical bar.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">num_trials</span> <span class="o">&lt;-</span> <span class="m">10</span>
</span></span><span class="line"><span class="cl"><span class="n">num_successes</span> <span class="o">&lt;-</span> <span class="m">7</span>
</span></span><span class="line"><span class="cl"><span class="n">prob_success</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="n">from</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">to</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span> <span class="c1">#manipulated variable </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">likelihood_distribution</span> <span class="o">&lt;-</span> <span class="nf">compute_binom_mass_density</span><span class="p">(</span><span class="n">num_trials</span> <span class="o">=</span> <span class="n">num_trials</span><span class="p">,</span> <span class="n">num_successes</span> <span class="o">=</span>  <span class="n">num_successes</span><span class="p">,</span> <span class="n">prob_success</span> <span class="o">=</span> <span class="n">prob_success</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#create data set for shaded rectangle that indicates the most likely value </span>
</span></span><span class="line"><span class="cl"><span class="c1">##index of highest probability </span>
</span></span><span class="line"><span class="cl"><span class="n">highest_number_ind</span> <span class="o">&lt;-</span> <span class="nf">which.max</span><span class="p">(</span><span class="n">likelihood_distribution</span><span class="o">$</span><span class="n">probability</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="c1">##most likely number of successes</span>
</span></span><span class="line"><span class="cl"><span class="n">maximum_likelihood_estimate</span> <span class="o">&lt;-</span> <span class="n">likelihood_distribution</span><span class="o">$</span><span class="n">prob_success[highest_number_ind]</span> 
</span></span><span class="line"><span class="cl"><span class="c1">##probability value of most likely number of successes</span>
</span></span><span class="line"><span class="cl"><span class="n">highest_prob</span> <span class="o">&lt;-</span> <span class="nf">max</span><span class="p">(</span><span class="n">likelihood_distribution</span><span class="o">$</span><span class="n">probability</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">rectangle_data</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="s">&#39;xmin&#39;</span> <span class="o">=</span> <span class="n">maximum_likelihood_estimate</span> <span class="o">-</span> <span class="m">.005</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">  <span class="s">&#39;xmax&#39;</span> <span class="o">=</span> <span class="n">maximum_likelihood_estimate</span> <span class="o">+</span> <span class="m">.005</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s">&#39;ymin&#39;</span> <span class="o">=</span> <span class="m">-0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s">&#39;ymax&#39;</span> <span class="o">=</span> <span class="n">highest_prob</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">likelihood_plot</span> <span class="o">&lt;-</span> <span class="nf">ggplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">likelihood_distribution</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">prob_success</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">probability</span><span class="p">))</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">  <span class="nf">geom_line</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span> <span class="s">&#34;#002241&#34;</span><span class="p">)</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">  <span class="nf">geom_rect</span><span class="p">(</span><span class="n">inherit.aes</span> <span class="o">=</span> <span class="bp">F</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">             <span class="n">fill</span> <span class="o">=</span> <span class="s">&#34;#002241&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span> <span class="o">=</span> <span class="n">rectangle_data</span><span class="p">,</span> <span class="n">mapping</span> <span class="o">=</span> <span class="nf">aes</span><span class="p">(</span><span class="n">xmin</span> <span class="o">=</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">ymin</span> <span class="o">=</span> <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">ymax</span><span class="p">))</span><span class="o">+</span> 
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="nf">scale_x_continuous</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="nf">bquote</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&#34;Probability of Heads (&#34;</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="s">&#34;)&#34;</span><span class="p">)),</span> 
</span></span><span class="line"><span class="cl">                       <span class="n">breaks</span> <span class="o">=</span> <span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0.10</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                       <span class="n">labels</span> <span class="o">=</span> <span class="n">scales</span><span class="o">::</span><span class="nf">number_format</span><span class="p">(</span><span class="n">accuracy</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">))</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="nf">scale_y_continuous</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="nf">bquote</span><span class="p">(</span><span class="nf">italic</span><span class="p">(</span><span class="s">&#34;L(&#34;</span><span class="p">)</span><span class="o">*</span><span class="nf">italic</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="s">&#34;| &#34;</span><span class="o">*</span><span class="nf">italic</span><span class="p">(</span><span class="s">&#34;h&#34;</span><span class="p">)</span><span class="o">==</span> <span class="n">.(num_successes</span><span class="p">)</span><span class="o">*</span><span class="s">&#34;,&#34;</span><span class="o">~</span><span class="nf">italic</span><span class="p">(</span><span class="s">&#34;n&#34;</span><span class="p">)</span> <span class="o">==</span> <span class="n">.(num_trials</span><span class="p">)</span><span class="o">*</span><span class="s">&#34;)&#34;</span><span class="p">),</span>  
</span></span><span class="line"><span class="cl">                       <span class="n">labels</span> <span class="o">=</span> <span class="n">scales</span><span class="o">::</span><span class="nf">number_format</span><span class="p">(</span><span class="n">accuracy</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                       <span class="n">breaks</span> <span class="o">=</span> <span class="nf">seq</span><span class="p">(</span><span class="n">from</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">to</span> <span class="o">=</span> <span class="m">.30</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="m">.10</span><span class="p">))</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">.30</span><span class="p">))</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="nf">theme_classic</span><span class="p">(</span><span class="n">base_family</span> <span class="o">=</span> <span class="s">&#34;Helvetica&#34;</span><span class="p">,</span> <span class="n">base_size</span> <span class="o">=</span> <span class="m">18</span><span class="p">)</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="nf">theme</span><span class="p">(</span><span class="n">axis.text.x</span> <span class="o">=</span> <span class="nf">element_text</span><span class="p">(</span><span class="n">face</span> <span class="o">=</span> <span class="nf">ifelse</span><span class="p">(</span><span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0.10</span><span class="p">)</span> <span class="o">==</span> <span class="n">maximum_likelihood_estimate</span><span class="p">,</span> <span class="s">&#34;bold&#34;</span><span class="p">,</span> <span class="s">&#34;plain&#34;</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">        <span class="n">text</span> <span class="o">=</span> <span class="nf">element_text</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span> <span class="s">&#34;#002241&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">axis.line</span> <span class="o">=</span> <span class="nf">element_line</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span> <span class="s">&#34;#002241&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">axis.ticks</span> <span class="o">=</span> <span class="nf">element_line</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span>  <span class="s">&#34;#002241&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">        <span class="n">axis.text</span> <span class="o">=</span> <span class="nf">element_text</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span> <span class="s">&#34;#002241&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">ggsave</span><span class="p">(</span><span class="n">filename</span> <span class="o">=</span> <span class="s">&#39;images/likelihood_plot.png&#39;</span><span class="p">,</span> <span class="n">plot</span> <span class="o">=</span> <span class="n">likelihood_plot</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="m">6</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="m">8</span><span class="p">)</span>
</span></span></code></pre></div><div class="figure">
  <div class="figDivLabel">
    <caption>
      <span class = 'figLabel'>Figure \ref{fig:likelihood-dist}<span> 
    </caption>
  </div>
   <div class="figTitle">
    <span>Likelihood Distribution With Seven Heads (h = 7) and Ten Coin Flips (n = 10)</span>
  </div>
    <img src="images/likelihood_plot.png" width="70%" height="70%"> 
  <div class="figNote">
      <span><em>Note. </em> Number emboldened on the x-axis indicates the value of <span class = "theta">&theta;</span> that is the maximum likelihood estimate, with the corresponding bar in dark blue indicating the corresponding value.</span> 
  </div>
</div>
<p>Although maximum likelihood estimates can be identified by creating likelihood distributions, this method is not efficient. Under many circumstances, creating such distributions is computationally demanding when a large range of parameter values must be considered. Even more important, however, many situations arise where many parameter values are estimated, and plotting in an <em>n</em>-dimensional space becomes very difficult when $n &gt; 3$, and virtually impossible when $n &gt; 5$. Thus, a more efficient method is needed to find maximum likelihood estimates.</p>
<h1 id="using-maximum-likelihood-estimation-to-find-the-most-likely-set-of-parameter-values">Using Maximum Likelihood Estimation to Find the Most Likely Set of Parameter Values</h1>
<p>Maximum likelihood estimation uses calculus to find a peak on the likelihood distribution. In mathematical parlance, maximum likelihood estimation solves for the parameter value where the derivative (i.e., rate of change) is zero. Assuming the likelihood only has one peak (i.e., it is convex), then the parameter value at the zero-derivative point is the maximum likelihood value. In mathematical notation, then, the maximum likelihood estimate, $\theta_{MLE}$, is the value of $\theta$ that maximizes the likelihood of the data such that</p>
<p>$$
\theta_{MLE} = \underset{\theta}{\arg\max}  L(\theta|D).
\label{eq:MLE-general}
$$
In the two sections that follow, I will apply maximum likelihood estimation for the binomial and gaussian cases.</p>
<h2 id="maximum-likelihood-estimation-for-the-binomial-case">Maximum Likelihood Estimation for the Binomial Case</h2>
<p>In the binomial case, there is only one parameter value of interest: the probability of heads, $\theta$. Thus, maximum likelihood estimation will find the value $\theta$ that maximizes the likelihood function,</p>
<p>$$\underset{\theta}{\arg\max}\text{ } L(\theta|h, n) = \underset{\theta}{\arg\max}\text{ }{n \choose h}(\theta)^{h}(1-\theta)^{(n-h)}. $$</p>
<p>Before computing the maximum likelihood estimate, however, it is important to apply a $\log$ transformation so that the computation of the derivative is greatly simplified and does not involve a lengthy application of the quotient, product,
and chain rules. In applying a $\log$ transformation to the likelihood function, the log-likelihood function shown below in Equation \ref{eq:binom-log-likelihood} results:</p>
<p>$$
\log[ L(\theta|h,n)] = \log {n \choose h}\ + h\log(\theta) + (n-h)\log(1-\theta)
\label{eq:binom-log-likelihood}
$$</p>
<h1 id="references">References</h1>

  

  










<section class="hugo-cite-bibliography">
  <dl>
    

      <div id="fine2019">

          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person">
    <span itemprop="familyName">Fine</span>,&#32; 
    <meta itemprop="givenName" content="Kimberly L." />
    K.   </span>&#32; 
  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
    <span itemprop="familyName">Suk</span>,&#32; 
    <meta itemprop="givenName" content="Hye Won" />
    H.   </span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person">
    <span itemprop="familyName">Grimm</span>,&#32; 
    <meta itemprop="givenName" content="Kevin J." />
    K.   </span>
  &#32;
    (<span itemprop="datePublished">2019</span>).
  &#32;<span itemprop="name">An examination of a functional mixed-effects modeling approach to the analysis of longitudinal data</span>.<i>
    <span itemprop="about">Multivariate Behavioral Research</span>,&#32;54(4)</i>,&#32;<span itemprop="pagination">475–491</span>.
  <a href="https://doi.org/10.1080/00273171.2018.1520626"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.1080/00273171.2018.1520626</a></span>






</dd>

      </div>
  </dl>
</section>



<h1 id="proof-pmf">Appendix A: Proof That the Binomial Function is a Probability Mass Function </h1>
<p>To prove that the binomial function is a probability mass function, two outcomes must be shown: 1) all probability values are non-negative and 2) the sum of all probabilities is 1.</p>
<p>For the first condition, the impossibility of negative values occurring in the binomial function becomes obvious when individually considering the binomial coefficient, $n \choose k$, and the binomial factors, $p^k (1-p)^{n-k}$. With respect to the binomial coefficient, $n \choose k$, it is always nonnegative because it is the product of two non-negative numbers; the number of trials, $n$, and the number of successes can never be negative. With respect to the binomial factors, the resulting value is always nonnegative because all the term are nonnegative; in addition to the number of trials and successes ($n, k$, respectively),  the probability of success and failure are also always nonnegative ($p, k \in [0,1]$). Therefore, probabilities can be conceptualized as the product of a nonnegative binomial coefficient and a nonnegative binomial factor, and so is alwasys nonnegative.</p>
<p>For the second condition, the equality stated below in Equation \ref{eq:binomial-sum-one} must be proven:</p>
<p>\begin{align}
1 = \sum^n_{k=0} {n \choose k} \theta^k(1-\theta)^{n-k}.<br>
\label{eq:binomial-sum-one}
\end{align}</p>
<p>Importantly, it can be proven that all probabilities sum to one by using the binomial theorem, which states below in Equation \ref{eq:binomial} that</p>
<p>\begin{align}
(a + b)^n =  \sum^n_{k=0} {n \choose k} a^k(b)^{n-k}.
\label{eq:binomial}
\end{align}</p>
<p>Given the striking resemblance between the binomial function in Equation \ref{eq:binomial-sum-one} and the binomial theorem in Equation \ref{eq:binomial-sum-one}, it is possible to restate the binomial theorem with respect to the variables in the binomial function. Specifically, we can let $a = p$ and $b = 1-p$, which returns the proof as shown below:</p>
<p>\begin{spreadlines}{0.5em}
\begin{align*}
(p + 1 -p)^n &amp;= \sum^n_{k=0} {n \choose k} p^k(1-p)^{n-k} \\ \nonumber
1 &amp;= \sum^n_{k=0} {n \choose k} p^k(1-p)^{n-k} \qquad\qquad _\square   \nonumber
\end{align*}
\end{spreadlines}</p>
<p>For a proof of the binomial theorem, see <a href="#proof-binomial">Appendix E</a>.</p>
<h1 id="proof-likelihood">Appendix B: Proof That Likelihoods are not Probabilities </h1>
<p>As a reminder, although the same formula is used to compute likelihoods and probabilities, the variables allowed to vary and those set to be fixed differ when computing likelihoods and probabilities. With probabilities, the parameters are fixed ($\theta$, $n$) and the data are varied ($h$; notice how, in Appendix A, probabilities were summed across all possible values of $h$). With likelihoods, however, the data are fixed ($h$) and the parameters are varied ($\theta$, $n$). For the current proof, it is sufficient to only allow $\theta$ to vary. To prove that likelihoods are not probabilities, we have to prove that likelihoods do not satisfy one of the two conditions required by probabilities (i.e., likelihoods can have negative values or likelihoods do not sum to one). Given that likelihoods are calculated with the same function as probabilities and probabilities can never be negative (see <a href="#proof-pmf">Appendix A</a>), likelihoods likewise can never ne negative. Therefore, to prove that likelihoods are not probabilities, we must prove that likelihoods do not always sum to 1. Thus, the following proposition must be proven:</p>
<p>$$
\int_0^1 L(\theta|h, n) \phantom{c} d\theta= \sum_{\theta = 0}^1{n \choose h} \theta^h(1 - \theta)^{n-h} \neq 1.
$$
In summing each likelihood for $\theta \in [0, 1]$, an equivalent calculation is to take the integral of the binomial function with respect to theta such that
$$
\begin{spreadlines}{0.5em}
\begin{align}
\int_0^1 L(\theta|h, n) \phantom{c} d\theta &amp;= \int_0^1 L(\theta|h ,n) \phantom{c} d\theta<br>
\label{eq:int-sum-likelihood}\\
&amp;= {n \choose h} \int_0^1 \theta^h(1-\theta)^{n-h}.
\label{eq:int-sum-likelihood-binomial}
\end{align}
\end{spreadlines}
$$
At this point, it is important to realize that $ \int_0^1 \theta^h(1-\theta)^{n-h}$ can be restated in terms of the beta function, $\mathrm{B}(x, y)$, which is shown below.
$$
\begin{spreadlines}{0.5em}
\begin{align}
\mathrm{B}(x, y) &amp;= \int_0^1 t^{x-1}(1-t){^{y-1}} \phantom{c} dt
\label{eq:beta-function} \\
\text{Let }&amp;t = \theta \nonumber \\
\mathrm{B}(x, y) &amp;= \int_0^1 \theta^{x-1}(1-\theta){^{y-1}} \phantom{c} d\theta \nonumber \\
\text{Let }&amp;x = h +1 \text{ and } y = n -h +1 \nonumber \\
\mathrm{B}(h+1, n-h+1) &amp;= \int_0^1 \theta^{h+1-1}(1-\theta){^{n-h+1-1}} \phantom{c} d\theta \nonumber \\
&amp;= \int_0^1 \theta^{h}(1-\theta){^{n-h}} \phantom{c} d\theta
\end{align}
\end{spreadlines}
$$
Therefore, the function in Equation \ref{eq:int-sum-likelihood-binomial} can be restated below in Equation \ref{eq:beta-restate} as
$$
\begin{align}
\int_0^1 L(\theta|h, n) \phantom{c} d\theta = {n \choose h} \mathrm{B}(h+1, n-h+1).
\label{eq:beta-restate}
\end{align}
$$
At this point, another proof becomes important because it allows us to express the beta function in terms of another function that will, ultimately, allow us to simplify Equation \ref{eq:beta-restate} and prove that likelihoods do not sum to one and are, therefore, not probabilities.  Specifically, the beta function, $\mathrm{B}(x, y)$ can be stated in terms of the gamma function $\Gamma$ such that</p>
<p>$$
\begin{align}
\mathrm{B}(x, y) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}.
\label{eq:beta-gamma-relation}
\end{align}
$$
For a proof of the beta-gamma relation, see <a href="#proof-beta-gamma">Appendix C</a>. Thus, Equation \ref{eq:beta-restate} can be stated in terms of the gamma function such that</p>
<p>$$
\begin{align}
\int_0^1 L(\theta|h, n) \phantom{c} d\theta  = {n \choose h} \frac{\Gamma(h+1)\Gamma(n-h+1)}{\Gamma(n+2)}.
\label{eq:binomial-gamma}
\end{align}
$$
One nice feature of the gamma function is that it can be stated as a factorial (for a proof, see <a href="#proof-gamma-factorial">Appendix D</a>) such that</p>
<p>$$
\begin{align}
\Gamma(x) = (x - 1)!.
\end{align}
$$
Given that the gamma function can be stated as a factorial, Equation \ref{eq:binomial-gamma} can be now be written with factorial terms and simplified to prove that likelihoods do not sum to one.</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align}
\int_0^1 L(\theta|h, n) \phantom{c} d\theta &amp;= \frac{n!}{h!(n-h)!}\frac{h!(n-h)!}{(n + 1)!} \nonumber \\
&amp;= \frac{n!}{(n + 1)!} \nonumber \\
&amp;= \frac{1}{n+1} \qquad\qquad _\square
\label{eq:likelihood-proof}<br>
\end{align}
\end{spreadlines}
$$</p>
<p>Therefore, binomial likelihoods sum to a multiple of $\frac{1}{1+n}$, where the multiple is the number of integration steps. The R code block below provided an example where the integral can be shown to be a multiple of the value in Equation \ref{eq:likelihood-proof}. In the example, the integral of the likelihood is taken over 100 equally spaced steps. Thus, the sum of likelihoods should be $100\frac{1}{1+n} = 9.09$, and this turns out to be true in the code below.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">num_trials</span> <span class="o">&lt;-</span> <span class="m">10</span> <span class="c1">#n</span>
</span></span><span class="line"><span class="cl"><span class="n">num_successes</span> <span class="o">&lt;-</span> <span class="m">7</span> <span class="c1">#h</span>
</span></span><span class="line"><span class="cl"><span class="n">prob_success</span> <span class="o">&lt;-</span> <span class="nf">seq</span><span class="p">(</span><span class="n">from</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">to</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">by</span> <span class="o">=</span> <span class="m">0.01</span><span class="p">)</span> <span class="c1">#theta; contains 100 values (i.e., there are 100 dtheta values)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">likelihood_distribution</span> <span class="o">&lt;-</span> <span class="nf">compute_binom_mass_density</span><span class="p">(</span><span class="n">num_trials</span> <span class="o">=</span> <span class="n">num_trials</span><span class="p">,</span> <span class="n">num_successes</span> <span class="o">=</span>  <span class="n">num_successes</span><span class="p">,</span> <span class="n">prob_success</span> <span class="o">=</span> <span class="n">prob_success</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">sum</span><span class="p">(</span><span class="n">likelihood_distribution</span><span class="o">$</span><span class="n">probability</span><span class="p">)</span> <span class="c1">#compute integral</span>
</span></span></code></pre></div><pre><code class='r-code'>[1] 9.09091
</code></pre>
<h1 id="proof-beta-gamma">Appendix C: Proof of Relation Between Beta and Gamma Functions </h1>
<p>Equation \ref{eq:beta-gamma-proof} below will be proven</p>
<p>$$
\begin{align}
\mathrm{B}(x, y) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}.
\label{eq:beta-gamma-proof}
\end{align}
$$
To begin, let&rsquo;s write out the expansions of the gamma function, $\Gamma(x)$, and the numerator of Equation \ref{eq:beta-gamma-proof}, \Gamma(x)\Gamma(y), where
$$
\begin{spreadlines}{0.5em}
\begin{align}
\Gamma(x) &amp;= \int^\infty_0 t^{x-1}e^{-t} \phantom{c} dt
\label{eq:gamma-function} \\
\Gamma(x)\Gamma(y) &amp;= \int^\infty_0 t^{x-1}e^{-t} \phantom{c} dt \int^\infty_0 s^{y-1}e^{-s} \phantom{c} ds.
\label{eq:gamma-numerator}
\end{align}
\end{spreadlines}
$$
Equation \ref{eq:gamma-function} shows the gamma function which will be useful as a reference and Equation \ref{eq:gamma-numerator} shows the expansion of the numerator in Equation \ref{eq:beta-gamma-proof}. To prove Equation \ref{eq:beta-gamma-proof}, we will begin by changing the variables of $s$ and $t$ in Equation \ref{eq:gamma-numerator} by reexpressing them in terms of $u$ and $v$. Importantly,  when changing variables in a double integral, the formula below in Equation \ref{eq:double-integral} must be followed:</p>
<p>$$
\begin{align}
\underset{G}{\iint} f(x, y) \text{ }dx \text{ }dy =  \underset{}{\iint}f(g(u, v), h(u, v))\det(\mathbf{J}(u, v)) \text{ }du \text{ }dv,
\label{eq:double-integral}
\end{align}
$$
where $|\mathbf{J}(u, v)|$ is the Jacobian of $u$ and $v$ (for a great explanation, see <a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/jacobian/v/the-jacobian-matrix">Jacobian</a> and <a href="https://www.youtube.com/watch?v=wUF-lyyWpUc">change of variables</a>). To apply Equation \ref{eq:double-integral}, we will first determine the expressions of $s$ and $t$ in terms of $u$ and $v$ to obtain $g(u,v)$ and $h(u,v)$, which are, respectively, provided below in Equation \ref{eq:s-rexp} and Equation \ref{eq:t-rexp}.</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align}
\text{Let } u &amp;= s + t, \text{  } v = \frac{t}{s+t} \nonumber \\
\text{then }  \text{ }s &amp;= u - t = u - uv = g(u, v)
\label{eq:s-rexp} \\
t &amp;= u - s = u - (u - uv) = uv = h(u, v).
\label{eq:t-rexp}
\end{align}
\end{spreadlines}
$$
With the expression for $g(u,v)$ and $h(u,v)$, the determinant of the Jacobian of $u$ and $v$ can now be computed, as shown below and provided in Equation \ref{eq:det-Jac}.
$$
\begin{spreadlines}{0.5em}
\begin{align}
\det\mathbf{J}(u, v) &amp;=
\det\begin{bmatrix}
\frac{\partial g}{\partial u} &amp; \frac{\partial g}{\partial v} \
\frac{\partial h}{\partial u} &amp; \frac{\partial h}{\partial v}
\end{bmatrix} \nonumber \\
&amp;= \det\begin{bmatrix}
1-v &amp; -u \
v &amp; u
\end{bmatrix} \nonumber \\
&amp;= (1 - v)u - (-uv) = u - uv + uv = u
\label{eq:det-Jac}
\end{align}
\end{spreadlines}
$$
With the $\det\mathbf{J}(u, v)$ computed, we can no express the new function with the changed variables, as shown below in Equation \ref{eq:gamma-reexp}.</p>
<p>$$
\begin{align}
\underset{G}{\iint} f(g(u, v), h(u, v))\det(\mathbf{J}(u, v)) \text{ }du \text{ }dv &amp;= \underset{R}{\iint} \Gamma(g(u, v))\Gamma(h(u,v))\det(\mathbf{J}(u, v)) \text{ }du \text{ }dv \nonumber \\
&amp;= \underset{R}{\iint}  uv^{x-1}e^{-uv}  (u - uv)^{y-1}e^{-(u - uv)}u d\text{ } u \text{ }dv \nonumber \\
&amp;= \underset{R}{\iint}  u^{x-1}v^{x-1} e^{-uv} (u(1 - v)^{y-1})e^{-(u - uv)}u \text{ } du\text{ }dv \nonumber \\
&amp;= \underset{R}{\iint}  u^{x-1}u^{y-1}u e^{-uv}e^{-u + uv}  v^{x-1} (1 - v)^{y-1} \text{ } du\text{ }dv \nonumber \\
&amp;= \underset{R}{\iint}  u^{x+y-1} e^{-u} v^{x-1} (1 - v)^{y-1} \text{ } du\text{ }dv
\label{eq:gamma-reexp}
\end{align}
$$
At this point, we need to determine the integration limits of $u$ and $v$ by evaluating them at the limits of $s$ and $t$, which is shown below.</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align}
&amp;\text{Recall } u = s + t, v = \frac{t}{s+t}, \text{ and } s,y \in[0, \infty] \nonumber \\
&amp;\text{If } s = 0 \Rightarrow u = t, v = 1 \nonumber \\
&amp;\phantom{If } s = \infty \Rightarrow u = \infty, v = 0 \nonumber \\
&amp;\phantom{If } t = 0 \Rightarrow u = s, v = 0 \nonumber \\
&amp;\phantom{If } t = \infty \Rightarrow u = \infty, v = 1 \nonumber
\end{align}
\end{spreadlines}
$$</p>
<p>Therefore, the original integration limits of 0 to $\infty$ of $s$ and $t$ produce integration limits 0 to $\infty$ for $u$ and 0 to 1 for $v$.  Recalling the gamma function (Equation \ref{eq:gamma-function} and the beta function (Equation \ref{eq:beta-function}, the beta function can now be expressed in terms of the gamma function, proving Equation \ref{eq:beta-gamma-proof}.</p>
<p>\begin{spreadlines}{0.5em}
\begin{align*}
\Gamma(x)\Gamma(y) &amp;= \int_0^1 \int_0^\infty u^{x+y-1} e^{-u} v^{x-1} (1 - v)^{y-1} ,du,dv \\
&amp;=  \int_0^\infty  u^{x+y-1} e^{-u}\text{ } du \int_0^1v^{x-1} (1 - v)^{y-1} ,dv \\
&amp;=  \Gamma(x + y)\mathrm{B}(x,y) \\
\mathrm{B}(x,y) &amp;= \frac{\Gamma(x)\Gamma(y)}{ \Gamma(x + y)} \qquad\qquad _\square
\end{align*}
\end{spreadlines}</p>
<h1 id="proof-gamma-factorial">Appendix D: Proof of Relation Between Gamma and Factorial Functions </h1>
<p>To prove the following proposition in Equation \ref{eq:gamma-factorial} that</p>
<p>$$
\begin{align}
\Gamma(x) &amp;= \int_0^\infty t^{x-1}e^{-t} dt =(x-1)!,
\label{eq:gamma-factorial}
\end{align}
$$
it is first helpful to prove the proposition below in Equation \ref{eq:gamma-pre-fac} that
$$
\begin{align}
\Gamma(\alpha + 1) &amp;= \alpha\Gamma(\alpha )
\label{eq:gamma-pre-fac}
\end{align}
$$
To prove Equation \ref{eq:gamma-pre-fac}, we first expand Equation \ref{eq:gamma-pre-fac} in Equation \ref{eq:gamma-expand} and then simplify Equation \ref{eq:gamma-expand} using integration by parts such that</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align}
\Gamma(\alpha + 1) &amp;= \int^\infty_0 t^\alpha e^{-t} \text{ } dt
\label{eq:gamma-expand} \\
\int u \text{ }dv &amp;= uv - \int v \text{ } du.<br>
\label{eq:int-parts} \\
\text{Let } u &amp;= t^\alpha \text{, } dv = e^{-t} \text{ } dt \text{, } \nonumber \\
du &amp;= \alpha t^{\alpha - 1}\text{, and } v = -e^{-t}. \nonumber\\
\int u \text{ }dv &amp;= -t^\alpha e^{-t}|^\infty_0 - \int^\infty_0(-e^{-t}) \alpha t^{\alpha - 1}
\label{eq:gamma-int-parts}
\end{align}
\end{spreadlines}
$$
To simplify Equation \ref{eq:gamma-int-parts}, I will first focus on the evaluation of $-t^\alpha e^{-t}$ between $\infty$ and $0$ below. At $t = \infty$,</p>
<p>$$
\begin{align*}
-t^\alpha e^{-t} = \frac{-\infty^{\alpha}}{e^\infty},
\label{eq:inf-eval}
\end{align*}
$$
and because $e^{\infty}$ approaches $\infty$ faster than $-\infty^\alpha$ approaches $-\infty$, Equation \ref{eq:inf-eval} becomes zero. At $t = 0$,
$$
\begin{align*}
-t^\alpha e^{-t} = \frac{-0^{\alpha}}{e^0} = \frac{0}{1} = 0.
\end{align*}
$$
Therefore, Equation \ref{eq:gamma-int-parts} simplifies to</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align*}
\int u \text{ }dv &amp;= 0 - 0 - \int^\infty_0(-e^{-t}) \alpha t^{\alpha - 1}  \\
&amp;= \alpha  \int^\infty_0t^{\alpha - 1} e^{-t}  \\
&amp;= \alpha \Gamma(\alpha) \qquad\qquad _\square
\end{align*}
\end{spreadlines}
$$
Having proven that $\Gamma(\alpha + 1) = \alpha\Gamma(\alpha)$, it becomes easy to prove Equation \ref{eq:gamma-factorial} which states that $\Gamma(x) = (x-1)!$. If I continue to expand the gamma function, $\Gamma(x-n)$, where $n = x -1$, I will obtain</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align*}
\Gamma(x) &amp;= (x-1)\Gamma(x-1) \\
\Gamma(x-1) &amp;= (x-2)\Gamma(x-2) \\
&amp;\vdots \\
\Gamma(x-n) &amp;= (1)\Gamma(1)
\end{align*}
\end{spreadlines}
$$
To evaluate $\Gamma(1)$, I write out its expansion and show that</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align*}
\Gamma(1) &amp;= \int^\infty_0t^{1-1}e^{-t} \text{ } dt \\
&amp;= e^{-t}|^\infty_0 \\
&amp;= e^{-\infty} - e^{0} \\
&amp;= 0 + 1 = 1\\
\end{align*}
\end{spreadlines}
$$
Therefore,  $\Gamma(x)$ expands to $(x-1)!$ because the last term will inevitably be  $1\times\Gamma(1) = 1$.</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align*}
\Gamma(x) = (x-1)(x-2)(x-3)&hellip;(x-n)\Gamma(x-n)  \\
=(x-1)(x-2)(x-3)&hellip;(1)\Gamma(1) \\
=(x-1)(x-2)(x-3)&hellip;(1)\Gamma(1) \\
=(x-1)! \qquad\qquad _\square
\end{align*}
\end{spreadlines}
$$</p>
<h1 id="proof-binomial">Appendix E: Proof of Binomial Theorem </h1>
<p>The binomial theorem provided below in Equation \ref{eq:binomial2} states that
$$
(x + y)^n = \sum^n_{k=0} {n \choose k} x^{n-k}y^k.
\label{eq:binomial2}
$$
I will prove the binomial theorem using induction. Thus, I will first prove the binomial theorem in a base where $n=1$ so that I can later generalize the proof with a larger number of $n+1$.  In the base case, the binomial theorem is proven such that
$$
\begin{spreadlines}{0.5em}
\begin{align*}
x + y &amp;= {1 \choose 0} x^{1-0}y^0 + {1 \choose 1} x^{1-1}y^1 \\
&amp;= x + y.
\end{align*}
\end{spreadlines}
$$
Now, I will prove the binomial theorem with $n + 1$. Thus,
$$
(x + y)^{n+1} = \sum^{n+1}_{k=0} {n+1 \choose k} x^{n+1-k}y^k.
\label{eq:binomial-induction}
$$
I now expand the left-hand side of Equation \ref{eq:binomial-induction}, to obtain</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align}
&amp;=(x + y)(x + y)^n \nonumber \\
&amp;= (x+y) \sum^n_{k=0} {n \choose k} x^{n-k}y^k \nonumber  \\
&amp;=x\sum^n_{k=0} {n \choose k} x^{n-k}y^k + y\sum^n_{k=0} {n \choose k} x^{n-k}y^k \nonumber \\
&amp;=\sum^n_{k=0} {n \choose k} x^{n+1-k}y^k + \sum^n_{k=0} {n \choose k} x^{n-k}y^{k+1} \nonumber \\
&amp;=\sum^n_{k=0} {n \choose k} x^{(n+1)-k}y^k + \sum^{n+1}_{k=1} {n \choose k-1} x^{n-(k-1)}y^{(k-1)+1}
\label{eq:binom-sums}
\end{align}
\end{spreadlines}
$$
Now I, respectively, remove $k = 0$ and $k = n+1$ from the first and second terms of Equation \ref{eq:binom-sums} so that the sums iterate over the same range of $k=1$ to $k = n$. I then use Pascal&rsquo;s rule to combine the two summations into one summation.</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align}
&amp;= \binom{n}{0} x^{n+1-0}y^0 + \binom{n}{n} x^{n-(n + 1 -1)}y^{(n+1-1)+1} + \sum^n_{k=1} \binom{n}{k} x^{(n+1)-k}y^k + \sum^n_{k=1} \binom{n}{k-1} x^{n-(k-1)}y^{(k-1)+1} \nonumber \\
&amp;=x^{n+1} + y^{n+1} + \sum^n_{k=1} {n \choose k} x^{n+1-k}y^k + \sum^n_{k=1} {n \choose k-1}  x^{n-k+1}y^{k}.
\label{eq:add-sums}
\end{align}
\end{spreadlines}
$$</p>
<p>I then apply Pascal&rsquo;s rule to simplify the addition of summations in Equation \ref{eq:add-sums} to obtain</p>
<p>$$
\begin{spreadlines}{0.5em}
\begin{align*}
&amp;=x^{n+1} + y^{n+1} + \sum^n_{k=1} {n+1 \choose k} x^{n+1-k}y^k \\
&amp;= {n+1 \choose 0} x^{n+1-0}y^0 + {n+1 \choose n+1} x^{n+1-(n+1)}y^{n+1} + \sum^n_{k=1} {n+1 \choose k} x^{n+1-k}y^k \\
&amp;= \sum^{n+1}_{k=0} {n+1 \choose k}x^{n-k+1}y^k \quad\quad _\square
\end{align*}
\end{spreadlines}
$$</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Discrete variables have a countable number of discrete values. In the current example with ten coin flips ($n = 10$), the number of heads is a discrete variable because the number of heads, $h$, has a countable number of outcomes, $h \in \{0, 1, 2, &hellip;, n\}$.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>It should be noted that Bayes&rsquo; formula can also be used to determine the value of $\theta$ that most likely produced the data. Instead of calculating, $P(h, n|\theta)$, however, Bayes&rsquo; formula uses prior information about an hypothesis to calculate the probability of $\theta$ given the data, $P(\theta|h, n)$ (for a review, see 





<span class="hugo-cite-intext"itemprop="citation">(<span class="hugo-cite-group">

          <a href="#fine2019"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Kimberly L."><span itemprop="familyName">Fine</span></span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person">
    <span itemprop="familyName">Fine</span>,&#32; 
    <meta itemprop="givenName" content="Kimberly L." />
    K.   </span>&#32; 
  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
    <span itemprop="familyName">Suk</span>,&#32; 
    <meta itemprop="givenName" content="Hye Won" />
    H.   </span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person">
    <span itemprop="familyName">Grimm</span>,&#32; 
    <meta itemprop="givenName" content="Kevin J." />
    K.   </span>
  &#32;
    (<span itemprop="datePublished">2019</span>).
  &#32;<span itemprop="name">An examination of a functional mixed-effects modeling approach to the analysis of longitudinal data</span>.<i>
    <span itemprop="about">Multivariate Behavioral Research</span>,&#32;54(4)</i>,&#32;<span itemprop="pagination">475–491</span>.
  <a href="https://doi.org/10.1080/00273171.2018.1520626"
     itemprop="identifier"
     itemtype="https://schema.org/URL">https://doi.org/10.1080/00273171.2018.1520626</a></span>






</span></span>)
</span>

<script type="text/javascript">

   document.addEventListener("DOMContentLoaded", function(e) {

      
     
      
      biblio = document.getElementsByClassName('hugo-cite-bibliography')[0];
      references = biblio.getElementsByTagName('div');

      
      for (let i = 0; i < references.length; i++) {

        
        

        
        biblio_entry_id = references[i].getAttribute('id');

       
       biblio_entry = document.getElementById(CSS.escape(biblio_entry_id));

        in_text_ref = document.querySelectorAll("a[href ^='#" + biblio_entry_id + "']");

        
            for (let i = 0; i < in_text_ref.length; i++) {

              
              family_names_raw = biblio_entry.querySelectorAll('[itemprop="familyName"]');  
              publish_dates = biblio_entry.querySelectorAll('[itemprop="datePublished"]')[0].textContent; 

              
              var family_names = [];

              for (let i = 0; i < family_names_raw.length; i++) {
                family_names.push(family_names_raw[i].textContent);
              }

               
              if(family_names.length > 2) {
              in_text_ref[i].querySelector('[itemprop="familyName"]').textContent = family_names[0] + ' et al. ' + publish_dates;
                }   else if (family_names.length == 2) {
                in_text_ref[i].querySelector('[itemprop="familyName"]').textContent = family_names[0] + ' & ' + family_names[1] + ', ' + publish_dates;
                }  else {
                in_text_ref[i].querySelector('[itemprop="familyName"]').textContent = family_names[0] + ', ' + publish_dates;
                }
            }
          } 
    });

</script>

).&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    </div>


    <div class = 'comments'>
    <script src="https://giscus.app/client.js"
        data-repo="sciarraseb/HugoWebsite"
        data-repo-id="R_kgDOIwrh4A"
        data-category="Comments"
        data-category-id="DIC_kwDOIwrh4M4CT1kX"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="https://cdn.jsdelivr.net/gh/sciarraseb/giscus@8cdfe5ff92c061ae8f8f3528299a0d5610530e8c/styles/themes/cobalt.css"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

    </div>

  </div>

  
    <div class = 'toc_scroll'>
      <div class = 'toc_container'>
        <nav class="tableContents">
          <span>Table of Contents</span>
        </nav>
      </div>
    </div>
    <button id="toc_button"><i class="fas fa-circle-right"></i></button>
    <button id="toc_button_bottom"><i class="fas fa-circle-down"></i></button>


  




</div>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="/js/mathjax_config.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-alpha.1/es5/tex-mml-chtml.js"></script>




        </main><footer class="site-footer">
  


  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

   <script src="/js/external_links.js"></script>
   <script src="/js/number_sections.js"></script>
   <script src="/js/table_contents.js"></script>
   <script src="/js/number_tables.js"></script>
   <script src="/js/number_figures.js"></script>
   <script src="/js/codefold.js"></script> 

</footer>





      </div>


    </body>

</html>

