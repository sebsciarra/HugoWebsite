weights -= learning_rate * gradient
# if desired, I expedite the algorithm by setting all the weights except the first one to the
# values obtained by the closed-form solution in each iteration
if expedite_algorithm:
weights[1:] = compute_weight_min_mse(data=data, poly_order=poly_order)[1:]
# update iteration number and check if difference in weights is less than epsilon
iteration += 1
diff = np.sum(np.square(learning_rate * gradient))
if return_log:
all_weights = np.vstack(tup=(all_weights, weights))
if return_log:
# create dataframe
col_names = ["{}{}".format('w', str(weight_num)) for weight_num in range(1, all_weights.shape[1] + 1)]
df_all_weights = pd.DataFrame(data=np.array(all_weights),
columns=col_names)
# insert iteration number
df_all_weights.insert(0, "iteration_num", df_all_weights.index)
return df_all_weights
else:
return weights
np.random.seed(27) #ensure reproducibility
initial_weights = np.random.uniform(low = 0, high = 1, size = 3)
initial_weights = np.insert(arr = initial_weights, obj=0, values=0)
data_gradient_weights = gradient_descent(data=data_emp_loss, initial_weights=initial_weights,  num_iterations=1100, learning_rate=0.0001, epsilon=1e-20, expedite_algorithm=True, return_log=True)
#obtain MSE values that correspond to each of the weight values in each gradient descent iteration
data_gradient_weights["mse_value"] = sml.compute_all_mse_loss_values(data = data_emp_loss, w_guess_list = data_gradient_weights["w1"])["mse_value"]
#only extract specific rows for plotting (rows 1, 101, and 301)
data_gradient_weights_plot = data_gradient_weights.iloc[[0,100, 300, data_gradient_weights.shape[0] - 1]]
#add a column for the latex math that will be printed
data_gradient_weights_plot["latex_math"] = ["${}^{}$".format('w', '{(' + str(i) + ')}') for i in data_gradient_weights_plot["iteration_num"]]
data_gradient_weights_plot["latex_math"].iloc[-1]  = data_gradient_weights_plot["latex_math"].iloc[-1] + " = $w_{MSE}$"
gradient_plot = (pt.ggplot(data = data_mse_values, mapping = pt.aes(x = "w_guess", y = "mse_value")) +
pt.geom_line(size = 1, color = "#002241") +
pt.geom_point(size = 2, data = data_gradient_weights_plot, mapping = pt.aes(x = "w1", y = "mse_value")) +
pt.geom_text(data=data_gradient_weights_plot,
mapping=pt.aes(x = "w1", y = "mse_value", label = "latex_math"),
nudge_y = [2.5e3, 2e3, 1.5e3, 2e3],
nudge_x = [0.05, 0.05, 0.05, 0.2],
size = 10, color = "#002241") +
pt.scale_x_continuous(limits = [0, 2.5],
breaks = np.insert(arr = np.arange(1, 2.50, 0.5), obj = 0, values = 0.64)) +
pt.labs(x = "Weight Value (w‚ÇÅ)", y = "Mean Squared Error") +
pt.theme_classic(base_family = 'Helvetica', base_size = 14) +
pt.theme(text = pt.element_text(color = "#002241"),
axis_line = pt.element_line(color = "#002241"),
axis_ticks = pt.element_line(color = "#002241"),
axis_ticks_minor_x = pt.element_blank()))
data_opt_error
data_opt_error
data_opt_error = sml.get_data_opt_error()
#create data set for pattern of empirical loss and generalization error that occurs with
#high variance
##high variance
training_sample_size = np.linspace(start = 1, stop = 1000, num = 1000)
#plateau point
M = 0.5
#satiation points and values (point at which curve reaches satiation value)
satiation_value_gen_error_high_var = 0.4
satiation_point_gen_error_high_var = 250
satiation_value_emp_loss_high_var = 0.15
satiation_point_emp_loss_high_var = 250
##high bias
satiation_value_gen_error_high_bias = 0.4
satiation_point_gen_error_high_bias = 250
satiation_value_emp_loss_high_bias = 0.37
satiation_point_emp_loss_high_bias = 175
#growth rate parameter value that curve reaches satiation value bu satiation point
a_gen_error_high_var = np.log(1 - satiation_value_gen_error_high_var)/-satiation_point_gen_error_high_var
a_emp_loss_high_var = np.log(1 - satiation_value_emp_loss_high_var)/-satiation_point_emp_loss_high_var
a_gen_error_high_bias = np.log(1 - satiation_value_gen_error_high_bias)/-satiation_point_gen_error_high_bias
a_emp_loss_high_bias = np.log(1 - satiation_value_emp_loss_high_bias)/-satiation_point_emp_loss_high_bias
##high variance
curve_gen_error_high_var = -M*(1 - np.exp(-a_gen_error_high_var*training_sample_size)) + 1
curve_emp_loss_high_var = M*(1 - np.exp(-a_emp_loss_high_var*training_sample_size))
##high bias
curve_gen_error_high_bias = -M*(1 - np.exp(-a_gen_error_high_bias*training_sample_size)) + 1
curve_emp_loss_high_bias = M*(1 - np.exp(-a_emp_loss_high_bias*training_sample_size))
data_emp_gen_error_curve = pd.DataFrame({'training_sample_size': training_sample_size,
'gen_error_high_var': curve_gen_error_high_var,
'emp_loss_high_var': curve_emp_loss_high_var,
'gen_error_high_bias': curve_gen_error_high_bias,
'emp_loss_high_bias': curve_emp_loss_high_bias})
cols_to_expand = data_emp_gen_error_curve.columns[1:5]
data_emp_gen_error_curve_long = data_emp_gen_error_curve.melt(id_vars = 'training_sample_size',
value_vars = cols_to_expand,
var_name = "error_type",
value_name = 'error_value')
# Define the regular expression pattern to match the second underscore and everything after it
pattern = r'^([^_]+_[^_]+)_(.*)$'
# Split each element using regular expressions
split_cols = data_emp_gen_error_curve_long['error_type'].str.extract(pattern)
#Create two new columns
data_emp_gen_error_curve_long["error_type"] = split_cols[0]
data_emp_gen_error_curve_long["problem"] = split_cols[1]
#Relevel error_type column
error_type_releveled = ["gen_error", "emp_loss"]
data_emp_gen_error_curve_long["error_type"] = data_emp_gen_error_curve_long['error_type'].astype('category').cat.reorder_categories(error_type_releveled)
def gen_sample_compute_weights(cov_matrix, mu, sample_size, seed, poly_order):
# Generate sample (note that seed value is set to iteration number to ensure reproducibility)
data_sample = generate_mult_trunc_normal(cov_matrix=cov_matrix, mu=mu,
sample_size=sample_size,
seed=seed)
# compute weights using closed-form solution (w = (XX)^-1Xy)
weights = compute_weight_min_mse(data=data_sample, poly_order=poly_order)
return weights
#for each polynomial order model from 1-9, generate 1000 data sets and apply the model on each data set
#compute the mean model and the difference in generalization error between each individual model
#and the mean model. Sample size for each generated data set will be fixed at 100.
#set the population means and SDs for weather and winemaking quality
mu = [5, 7]
sd = [1.2, 1.7]
#population correlation
rho_weather_winemaking =  0.35
cov_matrix = create_covariance_matrix(sd = sd, rho =  rho_weather_winemaking)
gen_sample_size = 1e5
#data used to compute generalization error
data_gen_error = generate_mult_trunc_normal(cov_matrix = cov_matrix, mu = mu, sd = sd,
rho = rho_weather_winemaking, sample_size = gen_sample_size)
data_gen_error.to_csv('data_gen_error_bv_tradeoff.csv')
#initiate variables
first_poly_order = 1
last_poly_order = 6
poly_model_orders = range(first_poly_order, last_poly_order+1)
num_iterations = 100
sample_size = 100
df_bias_var = pd.DataFrame(index = range(last_poly_order),
columns = ['bias_squared', 'variance', 'noise', 'mse', 'mse_sum'])
start_time = time.time()
for poly_order in poly_model_orders:
#numbers of columns corresponds to the 2*(poly_order) because there are
#two predictors (weather, winemaking & quality)
poly_order_weights = np.empty(shape = [num_iterations, poly_order*2])
for iteration in range(0, num_iterations):
new_sample_weights = gen_sample_compute_weights(cov_matrix = cov_matrix, mu = mu, sd = sd,
rho = rho_weather_winemaking, sample_size = sample_size,
seed = iteration, poly_order = poly_order)
poly_order_weights[iteration ,] = new_sample_weights
#1) Extract features and outcome from generalization error data
features = extract_feature_outcome_data(data = data_gen_error, poly_order = poly_order)["features"]
outcome = extract_feature_outcome_data(data = data_gen_error, poly_order = poly_order)["outcome"]
#2) Compute predictions of each sample model, the mean model (average sample risk minimizer), and
#the Bayes decision function
##compute average sample risk minimizer, \bar{f},
mean_weights = poly_order_weights.mean(axis = 0)
##Bayes decision function features + intercept
bayes_features = compute_bayes_features(data = data_gen_error)
bayes_intercept = compute_bayes_intercept(data = data_gen_error)
#predictions of each component
pred_mean = np.dot(a = features, b = mean_weights)
pred_sample = np.dot(a = features, b = np.transpose(poly_order_weights))
##recall that provide_weights() contains the population weights and has to be rescaled
pred_bayes = bayes_intercept + np.dot(a = bayes_features, b = compute_rescaling_factor() *
provide_weights())
#3) Compute components of mean squared error (bias squared, variance, and noise())
bias_squared = np.mean(np.square(pred_bayes - pred_mean))
variance = np.mean(np.square(np.transpose(pred_sample) - pred_mean))
noise = compute_bayes_risk(data = data_gen_error)
##also compute mean squared error using formula and, as a test, by adding components
mse = np.mean(np.square(pred_sample - outcome))
mse_sum = np.sum([bias_squared, variance, noise])
#add values for each component to dataframe
df_bias_var.iloc[poly_order - 1] = [bias_squared, variance, noise, mse, mse_sum]
end_time = time.time()
end_time - start_time #146.49782872200012
data_bv_tradeoff = pd.read_csv("data_bv_tradeoff.csv")
#initiate variables
first_poly_order = 1
last_poly_order = 6
poly_model_orders = range(first_poly_order, last_poly_order+1)
num_iterations = 100
sample_size = 100
df_bias_var = pd.DataFrame(index = range(last_poly_order),
columns = ['bias_squared', 'variance', 'noise', 'mse', 'mse_sum'])
start_time = time.time()
for poly_order in poly_model_orders:
#numbers of columns corresponds to the 2*(poly_order) because there are
#two predictors (weather, winemaking & quality)
poly_order_weights = np.empty(shape = [num_iterations, poly_order*2])
for iteration in range(0, num_iterations):
new_sample_weights = gen_sample_compute_weights(cov_matrix = cov_matrix, mu = mu, sd = sd,
rho = rho_weather_winemaking, sample_size = sample_size,
seed = iteration, poly_order = poly_order)
poly_order_weights[iteration ,] = new_sample_weights
#1) Extract features and outcome from generalization error data
features = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = poly_order)["features"]
outcome = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = poly_order)["outcome"]
#2) Compute predictions of each sample model, the mean model (average sample risk minimizer), and
#the Bayes decision function
##compute average sample risk minimizer, \bar{f},
mean_weights = poly_order_weights.mean(axis = 0)
##Bayes decision function features + intercept
bayes_features = compute_bayes_features(data = data_bv_tradeoff)
bayes_intercept = compute_bayes_intercept(data = data_bv_tradeoff)
#predictions of each component
pred_mean = np.dot(a = features, b = mean_weights)
pred_sample = np.dot(a = features, b = np.transpose(poly_order_weights))
##recall that provide_weights() contains the population weights and has to be rescaled
pred_bayes = bayes_intercept + np.dot(a = bayes_features, b = compute_rescaling_factor() *
provide_weights())
#3) Compute components of mean squared error (bias squared, variance, and noise())
bias_squared = np.mean(np.square(pred_bayes - pred_mean))
variance = np.mean(np.square(np.transpose(pred_sample) - pred_mean))
noise = compute_bayes_risk(data = data_bv_tradeoff)
##also compute mean squared error using formula and, as a test, by adding components
mse = np.mean(np.square(pred_sample - outcome))
mse_sum = np.sum([bias_squared, variance, noise])
#add values for each component to dataframe
df_bias_var.iloc[poly_order - 1] = [bias_squared, variance, noise, mse, mse_sum]
end_time = time.time()
end_time - start_time #146.49782872200012
def gen_sample_compute_weights(cov_matrix, mu, sample_size, seed, poly_order):
# Generate sample (note that seed value is set to iteration number to ensure reproducibility)
data_sample = generate_mult_trunc_normal(cov_matrix=cov_matrix, mu=mu,
sample_size=sample_size,
seed=seed)
# compute weights using closed-form solution (w = (XX)^-1Xy)
weights = compute_weight_min_mse(data=data_sample, poly_order=poly_order)
return weights
#initiate variables
first_poly_order = 1
last_poly_order = 6
poly_model_orders = range(first_poly_order, last_poly_order+1)
num_iterations = 100
sample_size = 100
df_bias_var = pd.DataFrame(index = range(last_poly_order),
columns = ['bias_squared', 'variance', 'noise', 'mse', 'mse_sum'])
start_time = time.time()
for poly_order in poly_model_orders:
#numbers of columns corresponds to the 2*(poly_order) because there are
#two predictors (weather, winemaking & quality)
poly_order_weights = np.empty(shape = [num_iterations, poly_order*2])
for iteration in range(0, num_iterations):
new_sample_weights = gen_sample_compute_weights(cov_matrix = cov_matrix, mu = mu,
sample_size = sample_size,
seed = iteration, poly_order = poly_order)
poly_order_weights[iteration ,] = new_sample_weights
#1) Extract features and outcome from generalization error data
features = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = poly_order)["features"]
outcome = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = poly_order)["outcome"]
#2) Compute predictions of each sample model, the mean model (average sample risk minimizer), and
#the Bayes decision function
##compute average sample risk minimizer, \bar{f},
mean_weights = poly_order_weights.mean(axis = 0)
##Bayes decision function features + intercept
bayes_features = compute_bayes_features(data = data_bv_tradeoff)
bayes_intercept = compute_bayes_intercept(data = data_bv_tradeoff)
#predictions of each component
pred_mean = np.dot(a = features, b = mean_weights)
pred_sample = np.dot(a = features, b = np.transpose(poly_order_weights))
##recall that provide_weights() contains the population weights and has to be rescaled
pred_bayes = bayes_intercept + np.dot(a = bayes_features, b = compute_rescaling_factor() *
provide_weights())
#3) Compute components of mean squared error (bias squared, variance, and noise())
bias_squared = np.mean(np.square(pred_bayes - pred_mean))
variance = np.mean(np.square(np.transpose(pred_sample) - pred_mean))
noise = compute_bayes_risk(data = data_bv_tradeoff)
##also compute mean squared error using formula and, as a test, by adding components
mse = np.mean(np.square(pred_sample - outcome))
mse_sum = np.sum([bias_squared, variance, noise])
#add values for each component to dataframe
df_bias_var.iloc[poly_order - 1] = [bias_squared, variance, noise, mse, mse_sum]
end_time = time.time()
end_time - start_time #146.49782872200012
df_bias_var
#initiate variables
first_poly_order = 1
last_poly_order = 6
poly_model_orders = range(first_poly_order, last_poly_order+1)
num_iterations = 100
sample_size = 100
df_bias_var = pd.DataFrame(index = range(last_poly_order),
columns = ['bias_squared', 'variance', 'noise', 'mse', 'mse_sum'])
poly_order_weights = np.empty(shape = [num_iterations, poly_order*2])
poly_order_weights
new_sample_weights = gen_sample_compute_weights(cov_matrix = cov_matrix, mu = mu,
sample_size = sample_size,
seed = 0, poly_order = 1)
new_sample_weights
poly_order_weights[0 ,] = new_sample_weights
features = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = 1)["features"]
outcome = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = 1)["outcome"]
features
poly_order_weights
f_bias_var = pd.DataFrame(index = range(last_poly_order),
columns = ['bias_squared', 'variance', 'noise', 'mse', 'mse_sum'])
for poly_order in poly_model_orders:
#numbers of columns corresponds to the 2*(poly_order) because there are
#two predictors (weather, winemaking & quality)
poly_order_weights = np.empty(shape = [num_iterations, poly_order*2])
for iteration in range(0, num_iterations):
new_sample_weights = gen_sample_compute_weights(cov_matrix = cov_matrix, mu = mu,
sample_size = sample_size,
seed = iteration, poly_order = poly_order)
poly_order_weights[iteration ,] = new_sample_weights
poly_order_weights
features = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = 1)["features"]
outcome = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = 1)["outcome"]
mean_weights = poly_order_weights.mean(axis = 0)
mean_weights
bayes_features = compute_bayes_features(data = data_bv_tradeoff)
bayes_intercept = compute_bayes_intercept(data = data_bv_tradeoff)
bayes_features = compute_bayes_features(data = data_bv_tradeoff)
bayes_features = sml.compute_bayes_features(data = data_bv_tradeoff)
bayes_features = sml.generate_bayes_features(data = data_bv_tradeoff)
bayes_intercept = sml.compute_bayes_intercept(data = data_bv_tradeoff)
#predictions of each component
pred_mean = np.dot(a = features, b = mean_weights)
pred_sample = np.dot(a = features, b = np.transpose(poly_order_weights))
features
pred_mean = np.dot(a = features.T, b = mean_weights)
mean_weights
features
features.shape
mean_weights
poly_order_weights
poly_order_weights.shape
poly_order_weights = np.empty(shape = [num_iterations, 1*2])
poly_order_weights
poly_order = 1
poly_order_weights = np.empty(shape = [num_iterations, poly_order*2])
for iteration in range(0, num_iterations):
new_sample_weights = gen_sample_compute_weights(cov_matrix = cov_matrix, mu = mu,
sample_size = sample_size,
seed = iteration, poly_order = poly_order)
poly_order_weights[iteration ,] = new_sample_weights
poly_order_weights
poly_order_weights.mean(axis = 1)
mean_weights = poly_order_weights.mean(axis = 0)
mean_weights
bayes_features = sml.generate_bayes_features(data = data_bv_tradeoff)
bayes_intercept = sml.compute_bayes_intercept(data = data_bv_tradeoff)
#predictions of each component
pred_mean = np.dot(a = features, b = mean_weights)
pred_sample = np.dot(a = features, b = np.transpose(poly_order_weights))
pred_bayes = bayes_intercept + np.dot(a = bayes_features, b = compute_rescaling_factor() *
provide_weights())
#3) Compute components of mean squared error (bias squared, variance, and noise())
bias_squared = np.mean(np.square(pred_bayes - pred_mean))
variance = np.mean(np.square(np.transpose(pred_sample) - pred_mean))
noise = compute_bayes_risk(data = data_bv_tradeoff)
##also compute mean squared error using formula and, as a test, by adding components
mse = np.mean(np.square(pred_sample - outcome))
mse_sum = np.sum([bias_squared, variance, noise])
#add values for each component to dataframe
df_bias_var.iloc[poly_order - 1] = [bias_squared, variance, noise, mse, mse_sum]
pred_bayes
bias_squared = np.mean(np.square(pred_bayes - pred_mean))
variance = np.mean(np.square(np.transpose(pred_sample) - pred_mean))
noise = compute_bayes_risk(data = data_bv_tradeoff)
mse = np.mean(np.square(pred_sample - outcome))
pred_sample
pred_sample.shape
variance = np.mean(np.square(np.transpose(pred_sample) - pred_mean))
variance
mse = np.mean(np.square(np.transpose(pred_sample)  - outcome))
mse_sum = np.sum([bias_squared, variance, noise])
mse
mse_sum
df_bias_var.iloc[poly_order - 1] = [bias_squared, variance, noise, mse, mse_sum]
start_time = time.time()
for poly_order in poly_model_orders:
#numbers of columns corresponds to the 2*(poly_order) because there are
#two predictors (weather, winemaking & quality)
poly_order_weights = np.empty(shape = [num_iterations, poly_order*2])
for iteration in range(0, num_iterations):
new_sample_weights = gen_sample_compute_weights(cov_matrix = cov_matrix, mu = mu,
sample_size = sample_size,
seed = iteration, poly_order = poly_order)
poly_order_weights[iteration ,] = new_sample_weights
#1) Extract features and outcome from generalization error data
features = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = poly_order)["features"]
outcome = extract_feature_outcome_data(data = data_bv_tradeoff, poly_order = poly_order)["outcome"]
#2) Compute predictions of each sample model, the mean model (average sample risk minimizer), and
#the Bayes decision function
##compute average sample risk minimizer, \bar{f},
mean_weights = poly_order_weights.mean(axis = 0)
##Bayes decision function features + intercept
bayes_features = sml.generate_bayes_features(data = data_bv_tradeoff)
bayes_intercept = sml.compute_bayes_intercept(data = data_bv_tradeoff)
#predictions of each component
pred_mean = np.dot(a = features, b = mean_weights)
pred_sample = np.dot(a = features, b = np.transpose(poly_order_weights))
##recall that provide_weights() contains the population weights and has to be rescaled
pred_bayes = bayes_intercept + np.dot(a = bayes_features, b = compute_rescaling_factor() *
provide_weights())
#3) Compute components of mean squared error (bias squared, variance, and noise())
bias_squared = np.mean(np.square(pred_bayes - pred_mean))
variance = np.mean(np.square(np.transpose(pred_sample) - pred_mean))
noise = compute_bayes_risk(data = data_bv_tradeoff)
##also compute mean squared error using formula and, as a test, by adding components
mse = np.mean(np.square(np.transpose(pred_sample)  - outcome))
mse_sum = np.sum([bias_squared, variance, noise])
#add values for each component to dataframe
df_bias_var.iloc[poly_order - 1] = [bias_squared, variance, noise, mse, mse_sum]
end_time = time.time()
end_time - start_time #146.49782872200012
df_bias_var.insert(0, "poly_order", df_bias_var.index + 1)
#convert to long format
df_bias_var_long = df_bias_var.melt(id_vars = 'poly_order',
value_vars = ["bias_squared", "variance", "noise", "mse"],
var_name = "component", value_name = "value")
#reordered levels of of component column
comp_releveled = ["mse", "bias_squared", "variance", "noise"]
df_bias_var_long['poly_order'] = df_bias_var_long['poly_order'].astype('int8')
df_bias_var_long['component'] = df_bias_var_long['component'].astype('category').cat.reorder_categories(comp_releveled)
df_bias_var_long['value'] = df_bias_var_long['value'].astype('float64')
# Define color palette for each error type
color_palette = {'mse': '#002241',
'bias_squared': 'blue',
'variance': '#2171B5',
'noise': '#9ECAE1'}
plot_bias_var = (pt.ggplot(data = df_bias_var_long,
mapping = pt.aes(x = "poly_order", y = "value",
group = "component", color = "component")) +
pt.geom_line(size = 1) +
pt.scale_x_continuous(name = "Order of Polynomial Model", breaks = range(1, 7)) +
pt.scale_y_continuous(name="Value (Mean of Squared Error)", limits = [0, 1.5],
breaks = np.arange(0, 1.6, 0.5)) +
#add custom colour palette + change legend labels
pt.scale_color_manual(name = "Component",
values = color_palette,
labels = {'mse': 'Mean squared error',
'bias_squared': 'Bias$^2$',
'variance': 'Variance',
'noise': 'Noise'}) +
#custom styling
pt.theme_classic(base_family = 'Helvetica', base_size = 14) +
pt.theme(legend_text = pt.element_text(size = 14),
legend_title = pt.element_text(size = 15),
axis_title  = pt.element_text(size = 15),
axis_text = pt.element_text(size = 14, color = "#002241"),
text = pt.element_text(color = "#002241"),
axis_line = pt.element_line(color = "#002241"),
axis_ticks = pt.element_line(color =  "#002241"),
axis_ticks_minor_x = pt.element_blank())
)
import smltheory as sml
sml.version('smltheory')
data_gradient_weights = sml.gradient_descent(data=data_emp_loss, initial_weights=initial_weights,  num_iterations=1100, learning_rate=0.0001, epsilon=1e-20, expedite_algorithm=True, return_log=True)
from trun_mvnt import trun_mvnt as tvm
import numpy as np
import pandas as pd
import sklearn
from sklearn.metrics import mean_squared_error
import statsmodels.formula.api as smf
from functools import partial
import plotnine as pt
import concurrent.futures
import smltheory as sml
#data for empirical loss
data_emp_loss = sml.get_data_emp_loss()
#data for generalization error
data_gen_error= sml.get_data_gen_error()
py_packages <- c('numpy', 'pandas', 'scikit-learn', "plotnine", "statsmodels", "smltheory")
conda_install(envname = 'blog_posts', packages = py_packages, pip=T)
#load packages
library(easypackages)
packages <- c('devtools','tidyverse', 'reticulate', 'RColorBrewer', 'ggforce', 'latex2exp', 'ggbrace')
libraries(packages)
knitr::opts_chunk$set(comment = NA, echo = TRUE, eval = TRUE, warning = FALSE, message=FALSE)
# knitr hook to use Hugo highlighting options
knitr::knit_hooks$set(
source = function(x, options) {
hlopts <- options$hlopts
paste0(
"```", "r ",
if (!is.null(hlopts)) {
paste0("{",
glue::glue_collapse(
glue::glue('{names(hlopts)}={hlopts}'),
sep = ","
), "}"
)
},
"\n", glue::glue_collapse(x, sep = "\n"), "\n```\n"
)
}
)
chunk_class <- function(before, options, envir) {
class_name = options$class_name
if (!is.null(before)) {
lines <- unlist(strsplit(x = before, split = "\n")) #separate lines of code at \n
n <- length(lines)  #determines numbers of lines
#if (line_numbers) {
res <- paste0("<pre><code class='", class_name, "'>", before, "</code></pre>")
#paste0("<pre><code class='", class_name, "'>", before, "</code></pre>")
#}
#res <- paste0("<pre>", paste0("<span class='line-number'>", 1:n,
#"</span><code class ='", class_name, "'>", lines, "</code>"), "</pre>")
}
return(res)
}
knitr::knit_hooks$set(output = chunk_class, preserve = TRUE)
#knitr::knit_hooks$set(output = function(x, options) {
#  paste(c("<pre><code class = 'r-code'>",
#        gsub('^## Error', '**Error**', x),
#        '</pre></code>'), collapse = '\n')
#})
options(reticulate.autocomplete = TRUE)
#create and use conda environment
#conda_create(envname = 'blog_posts',  python_version = '3.10.11')
use_condaenv(condaenv = 'blog_posts')
#install packages in conda environment
#py_packages <- c('numpy', 'pandas', 'scikit-learn', "plotnine", "statsmodels", "smltheory")
#conda_install(envname = 'blog_posts', packages = py_packages, pip=T)
#install_tensorflow(method = 'conda', envname = 'blog_posts')#can take long time (~10 minutes)
#useful for checking what packages are loaded
#py_list_packages(envname = 'blog_posts', type = 'conda')
#pandoc content/technical_content/understanding_ML/refs.bib -t csljson -o content/technical_content/understanding_ML/refs.json
#useful for checking what packages are loaded
py_list_packages(envname = 'blog_posts', type = 'conda')
reticulate::repl_python()
